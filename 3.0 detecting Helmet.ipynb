{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video source opened successfully.\n",
      "Frame Width: 640.0\n",
      "Frame Height: 480.0\n",
      "FPS: 30.00003000003\n",
      "Predictions: {0: 'HELMET', 1: 'NO HELMET'}\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model-006.keras')\n",
    "\n",
    "face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "source = cv2.VideoCapture(0, cv2.CAP_DSHOW)  \n",
    "\n",
    "source.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "source.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "source.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "labels_dict = {0: 'HELMET', 1: 'NO HELMET'}\n",
    "color_dict = {0: (0, 255, 0), 1: (0, 0, 255)}\n",
    "\n",
    "if not source.isOpened():\n",
    "    print(\"Error: Could not open video source.\")\n",
    "else:\n",
    "    print(\"Video source opened successfully.\")\n",
    "    print(\"Frame Width:\", source.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    print(\"Frame Height:\", source.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(\"FPS:\", source.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "print(f\"Predictions: {labels_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m      8\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m----> 9\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mface_clsfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, h \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     12\u001b[0m     offset_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.50\u001b[39m) \n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\objdetect\\src\\cascadedetect.cpp:1689: error: (-215:Assertion failed) !empty() in function 'cv::CascadeClassifier::detectMultiScale'\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, img = source.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture image\")\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    face = face_clsfr.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for x, y, w, h in faces:\n",
    "        offset_y = int(h * 0.50) \n",
    "        y = max(0, y - offset_y)\n",
    "        h = h + offset_y\n",
    "\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(face_img, (100, 100))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized, (1, 100, 100, 1))\n",
    "        \n",
    "        result = model.predict(reshaped)\n",
    "        helmet_prob = result[0][0]\n",
    "        no_helmet_prob = result[0][1]\n",
    "\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color_dict[label], 2)\n",
    "        \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.6\n",
    "        font_thickness = 1\n",
    "        text_color = (255, 255, 255)\n",
    "        outline_color = (0, 0, 0) \n",
    "        \n",
    "        label_text = f\"{labels_dict[label]}: {helmet_prob * 100:.2f}% | {no_helmet_prob * 100:.2f}%\"\n",
    "        \n",
    "        cv2.putText(img, label_text, (x+5, y-15), font, font_scale, outline_color, font_thickness+2)\n",
    "        cv2.putText(img, label_text, (x+5, y-15), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    cv2.imshow('LIVE HELMET DETECTION', img)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == 27:  \n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "source.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video source opened successfully.\n",
      "Frame Width: 640.0\n",
      "Frame Height: 480.0\n",
      "FPS: 30.00003000003\n",
      "Predictions: {0: 'HELMET', 1: 'NO HELMET'}\n",
      "1/1 [==============================] - 0s 279ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     35\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m---> 36\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mface_clsfr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y, w, h \u001b[38;5;129;01min\u001b[39;00m faces:\n\u001b[0;32m     39\u001b[0m     offset_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(h \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.50\u001b[39m) \n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from keras.models import load_model\n",
    "# import time\n",
    "\n",
    "# model = load_model('model-006.model')\n",
    "\n",
    "# face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# source = cv2.VideoCapture(0, cv2.CAP_DSHOW)  \n",
    "\n",
    "# source.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "# source.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "# source.set(cv2.CAP_PROP_FPS, 30)\n",
    "\n",
    "# labels_dict = {0: 'HELMET', 1: 'NO HELMET'}\n",
    "# color_dict = {0: (0, 255, 0), 1: (0, 0, 255)}\n",
    "\n",
    "# if not source.isOpened():\n",
    "#     print(\"Error: Could not open video source.\")\n",
    "# else:\n",
    "#     print(\"Video source opened successfully.\")\n",
    "#     print(\"Frame Width:\", source.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "#     print(\"Frame Height:\", source.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "#     print(\"FPS:\", source.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# print(f\"Predictions: {labels_dict}\")\n",
    "\n",
    "# while True:\n",
    "#     ret, img = source.read()\n",
    "#     if not ret:\n",
    "#         print(\"Failed to capture image\")\n",
    "#         time.sleep(0.1)\n",
    "#         continue\n",
    "\n",
    "#     gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_clsfr.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "#     for x, y, w, h in faces:\n",
    "#         offset_y = int(h * 0.50) \n",
    "#         y = max(0, y - offset_y)\n",
    "#         h = h + offset_y\n",
    "\n",
    "#         face_img = gray[y:y+h, x:x+w]\n",
    "#         resized = cv2.resize(face_img, (100, 100))\n",
    "#         normalized = resized / 255.0\n",
    "#         reshaped = np.reshape(normalized, (1, 100, 100, 1))\n",
    "        \n",
    "#         result = model.predict(reshaped)\n",
    "#         helmet_prob = result[0][0]\n",
    "#         no_helmet_prob = result[0][1]\n",
    "\n",
    "#         label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "#         cv2.rectangle(img, (x, y), (x+w, y+h), color_dict[label], 2)\n",
    "        \n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         font_scale = 0.6\n",
    "#         font_thickness = 1\n",
    "#         text_color = (255, 255, 255)\n",
    "#         outline_color = (0, 0, 0) \n",
    "        \n",
    "#         label_text = f\"{labels_dict[label]}: {helmet_prob * 100:.2f}% | {no_helmet_prob * 100:.2f}%\"\n",
    "        \n",
    "#         cv2.putText(img, label_text, (x+5, y-15), font, font_scale, outline_color, font_thickness+2)\n",
    "#         cv2.putText(img, label_text, (x+5, y-15), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "#     cv2.imshow('LIVE HELMET DETECTION', img)\n",
    "#     key = cv2.waitKey(1)\n",
    "\n",
    "#     if key == 27:  \n",
    "#         break\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "# source.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
